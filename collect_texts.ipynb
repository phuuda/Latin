{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all author links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "home = \"http://latin.packhum.org/browse\" # page with all author links\n",
    "\n",
    "def get_links(soup):\n",
    "    \n",
    "    for tag in soup.findAll('a'): # find tags that contain links\n",
    "        link = tag['href']\n",
    "        match = re.findall('/author/[0-9]+', link) # find all links in this format\n",
    "        if match:\n",
    "    \n",
    "            np_link = 'http://latin.packhum.org' + match[0] # create whole link\n",
    "            \n",
    "            print(np_link)\n",
    "            \n",
    "            author_name = tag.findAll('span')\n",
    "            author_name = author_name[0].text # get author name\n",
    "        \n",
    "            if np_link not in author_links:\n",
    "                author_links.append([author_name, np_link])\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    }
   ],
   "source": [
    "author_links = [] # each element: [author name, author link]\n",
    "\n",
    "req = requests.get(\"http://latin.packhum.org/browse\") # the page where we search for author links\n",
    "soup = BeautifulSoup(req.text, 'lxml')\n",
    "\n",
    "get_links(soup) # calls function to collect all author links\n",
    "\n",
    "print(len(author_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all text links (to page 1) from \"author_links\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_links(soup, author_link):\n",
    "    \n",
    "    for tag in soup.findAll('a'):\n",
    "        link = tag['href']\n",
    "        match = re.findall('/loc/.*$', link) # finds all links to texts\n",
    "        if match:\n",
    "        \n",
    "            text_name = tag.findAll('span')\n",
    "            text_name = text_name[1].text\n",
    "            \n",
    "            print(match[0] + ' ' + author_name + ' ' + text_name)\n",
    "            \n",
    "            np_link = 'http://latin.packhum.org' + match[0] # create whole link\n",
    "            print(np_link)\n",
    "            \n",
    "            if np_link not in text_links:\n",
    "                text_links.append([author_name, text_name, np_link])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n"
     ]
    }
   ],
   "source": [
    "text_links = [] # each element: [author name, text name, link to page 1]\n",
    "\n",
    "for i in author_links:\n",
    "    author_name = i[0]\n",
    "    author_link = i[1]\n",
    "    req = requests.get(author_link) # the page where we search for text links\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    \n",
    "    get_text_links(soup, author_link) # calls function to collect all links to texts\n",
    "    \n",
    "print(len(text_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all texts (every page) from \"text_links\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(page_link): # returns text from current page\n",
    "    \n",
    "    req = requests.get(page_link)\n",
    "    soup = BeautifulSoup(req.text, 'lxml') # current page soup\n",
    "    \n",
    "    page_text = '' # this is needed. if it remains empty, we know we reached the end of the text pages\n",
    "        \n",
    "    for tag in soup.findAll('table'):\n",
    "\n",
    "        for tr in tag.findAll('tr'): # find all tags with text\n",
    "            line = tr.text\n",
    "\n",
    "            while '\\n' in line: # delete empty lines \n",
    "                line = line.replace('\\n', '')\n",
    "\n",
    "            while '                           ' in line: # delete extra spaces before numeration\n",
    "                line = line.replace('                           ', '\\t\\t')\n",
    "            line = line.lstrip() # delete spaces from beginning of string\n",
    "            line = line.rstrip() # delete spaces from end of string\n",
    "            \n",
    "            page_text += line\n",
    "            page_text += '\\n'\n",
    "            \n",
    "    return page_text\n",
    "\n",
    "def get_whole_text(author, text_name, text_link): # finds each page of a text, compiles complete text\n",
    "    \n",
    "    file_name = author + ' - ' + text_name + '.txt' # unique name for text    \n",
    "    f = open('text files/' + file_name, 'w', encoding = 'utf-8') # creates new file for a text\n",
    "    \n",
    "    # all files will be saved in 'text files' folder\n",
    "    # you need to create this folder first\n",
    "    \n",
    "    print(author + ' - ' + text_name)\n",
    "    \n",
    "    whole_text = ''\n",
    "   \n",
    "    template = text_link[:-1]\n",
    "    index = 0\n",
    "    \n",
    "    current_page = template + str(index) # 1st page link\n",
    "    print(current_page)\n",
    "\n",
    "    page_text = get_text(current_page) # get text from 1st page\n",
    "    whole_text += page_text # add page text to total text\n",
    "\n",
    "    while page_text: # keep getting text if next page exists\n",
    "\n",
    "        index += 1\n",
    "        current_page = template + str(index) # link to next page\n",
    "        print(current_page)\n",
    "\n",
    "        page_text = get_text(current_page) # get text from next page (will be empty if there's no next page)\n",
    "        whole_text += page_text\n",
    "            \n",
    "    f.write(whole_text)          \n",
    "    f.close()\n",
    "    \n",
    "    all_texts.append([author, text_name, whole_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n"
     ]
    }
   ],
   "source": [
    "all_texts = [] # each element: [author, text name, text]\n",
    "\n",
    "for item in text_links: # compile every text\n",
    "    author = item[0]\n",
    "    text_name = item[1]\n",
    "    text_link = item[2]\n",
    "    \n",
    "    get_whole_text(author, text_name, text_link) # creates new file for each text\n",
    "                                                 # the texts are now also in 'all_texts'\n",
    "\n",
    "print(len(all_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save every text into \"all_texts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('all_texts.txt', 'w', encoding = 'utf-8') # new file for all texts\n",
    "\n",
    "for item in all_texts:\n",
    "    author = item[0]\n",
    "    text_name = item[1]\n",
    "    whole_text = item[2]\n",
    "    \n",
    "    f1.write(author)\n",
    "    f1.write('\\n')\n",
    "    f1.write(text_name)\n",
    "    f1.write('\\n\\n')\n",
    "    f1.write(whole_text)\n",
    "    f1.write('-----------------------------------------------')\n",
    "    f1.write('\\n')\n",
    "    \n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open text files, load into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "texts = [] # each element is [author and text name, text]\n",
    "\n",
    "path = 'text files/' # path to folder with texts\n",
    "\n",
    "for filename in glob.glob(os.path.join(path, '*.txt')): # finds all .txt in 'text files' folder\n",
    "\n",
    "    f = open(filename, 'r', encoding = 'utf-8')\n",
    "    text = f.read()\n",
    "    \n",
    "    texts.append([filename, text])\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "print(len(texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
